{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8918468,"sourceType":"datasetVersion","datasetId":5363365},{"sourceId":8926167,"sourceType":"datasetVersion","datasetId":5369192}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Hydrangea Image Classification\n---","metadata":{}},{"cell_type":"markdown","source":"# 0. Introduction\n\nAlthough hydrangeas have felt familiar to me since childhood, I recently noticed that there are so many different varieties. Fascinated by their beauty and playfulness, I've decided to make a classification model of hydrangea's images. \n\nThis project uses a dataset collected by scraping Google Images. As a beginner in ML/DS, I hope to share parts of my trial-and-error process with this limited dataset to receive feedback on my workflow and to potentially benefit other learners.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, foldernames, filenames in os.walk('/kaggle/input'): #path to input data\n    for foldername in foldernames:\n        print(os.path.join(dirname, foldername))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-29T00:57:40.236630Z","iopub.execute_input":"2024-07-29T00:57:40.236985Z","iopub.status.idle":"2024-07-29T00:57:41.297773Z","shell.execute_reply.started":"2024-07-29T00:57:40.236953Z","shell.execute_reply":"2024-07-29T00:57:41.296709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tensorflow==2.16.1","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip show tensorflow","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip show keras","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Data Preparation","metadata":{}},{"cell_type":"code","source":"import tensorflow\nimport keras\nimport numpy as np\nfrom matplotlib import pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2024-07-26T09:45:21.686416Z","iopub.execute_input":"2024-07-26T09:45:21.686899Z","iopub.status.idle":"2024-07-26T09:45:21.691413Z","shell.execute_reply.started":"2024-07-26T09:45:21.686870Z","shell.execute_reply":"2024-07-26T09:45:21.690373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1-1 Load Data","metadata":{}},{"cell_type":"code","source":"data_dir = '/kaggle/input/hydrangea-dataset-1'\ndata = keras.utils.image_dataset_from_directory(data_dir, label_mode=\"categorical\")\nclass_names = data.class_names","metadata":{"execution":{"iopub.status.busy":"2024-07-26T07:35:08.596075Z","iopub.execute_input":"2024-07-26T07:35:08.596443Z","iopub.status.idle":"2024-07-26T07:35:10.385075Z","shell.execute_reply.started":"2024-07-26T07:35:08.596413Z","shell.execute_reply":"2024-07-26T07:35:10.384261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_iterator = data.as_numpy_iterator()\nbatch = data_iterator.next()","metadata":{"execution":{"iopub.status.busy":"2024-07-26T07:35:13.405782Z","iopub.execute_input":"2024-07-26T07:35:13.406573Z","iopub.status.idle":"2024-07-26T07:35:13.601211Z","shell.execute_reply.started":"2024-07-26T07:35:13.406534Z","shell.execute_reply":"2024-07-26T07:35:13.599994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(batch) ) #returns 2: image and labels\nprint(batch[0].shape) #Images represented as numbpy arrays","metadata":{"execution":{"iopub.status.busy":"2024-07-26T07:35:14.771464Z","iopub.execute_input":"2024-07-26T07:35:14.772116Z","iopub.status.idle":"2024-07-26T07:35:14.777000Z","shell.execute_reply.started":"2024-07-26T07:35:14.772078Z","shell.execute_reply":"2024-07-26T07:35:14.776108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch[0].min()","metadata":{"execution":{"iopub.status.busy":"2024-07-26T07:35:16.822718Z","iopub.execute_input":"2024-07-26T07:35:16.823605Z","iopub.status.idle":"2024-07-26T07:35:16.831675Z","shell.execute_reply.started":"2024-07-26T07:35:16.823570Z","shell.execute_reply":"2024-07-26T07:35:16.830858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch[0].max()","metadata":{"execution":{"iopub.status.busy":"2024-07-26T07:35:18.511401Z","iopub.execute_input":"2024-07-26T07:35:18.512281Z","iopub.status.idle":"2024-07-26T07:35:18.520624Z","shell.execute_reply.started":"2024-07-26T07:35:18.512250Z","shell.execute_reply":"2024-07-26T07:35:18.519536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1-2 Scale Data","metadata":{}},{"cell_type":"code","source":"data = data.map(lambda x, y: (x/255, y))\nscale_iterator = data.as_numpy_iterator().next()","metadata":{"execution":{"iopub.status.busy":"2024-07-26T07:35:20.578971Z","iopub.execute_input":"2024-07-26T07:35:20.579387Z","iopub.status.idle":"2024-07-26T07:35:20.864757Z","shell.execute_reply.started":"2024-07-26T07:35:20.579356Z","shell.execute_reply":"2024-07-26T07:35:20.863926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scale_iterator[0].min()","metadata":{"execution":{"iopub.status.busy":"2024-07-26T07:35:21.906907Z","iopub.execute_input":"2024-07-26T07:35:21.907288Z","iopub.status.idle":"2024-07-26T07:35:21.914976Z","shell.execute_reply.started":"2024-07-26T07:35:21.907259Z","shell.execute_reply":"2024-07-26T07:35:21.914073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scale_iterator[0].max()","metadata":{"execution":{"iopub.status.busy":"2024-07-26T07:35:23.114151Z","iopub.execute_input":"2024-07-26T07:35:23.114814Z","iopub.status.idle":"2024-07-26T07:35:23.123141Z","shell.execute_reply.started":"2024-07-26T07:35:23.114779Z","shell.execute_reply":"2024-07-26T07:35:23.122095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1-3 Split Data","metadata":{}},{"cell_type":"code","source":"len(data)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T07:35:25.958219Z","iopub.execute_input":"2024-07-26T07:35:25.958596Z","iopub.status.idle":"2024-07-26T07:35:25.965309Z","shell.execute_reply.started":"2024-07-26T07:35:25.958569Z","shell.execute_reply":"2024-07-26T07:35:25.964453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_size = int(len(data)*.7)+1\nval_size = int(len(data)*.2)\ntest_size = int(len(data)*.1)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T07:35:27.029758Z","iopub.execute_input":"2024-07-26T07:35:27.030408Z","iopub.status.idle":"2024-07-26T07:35:27.035743Z","shell.execute_reply.started":"2024-07-26T07:35:27.030374Z","shell.execute_reply":"2024-07-26T07:35:27.034884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_size+val_size+test_size \n# make sure this matches the \"len(data)\" to make full use of your data","metadata":{"execution":{"iopub.status.busy":"2024-07-26T07:35:28.771504Z","iopub.execute_input":"2024-07-26T07:35:28.772098Z","iopub.status.idle":"2024-07-26T07:35:28.777518Z","shell.execute_reply.started":"2024-07-26T07:35:28.772065Z","shell.execute_reply":"2024-07-26T07:35:28.776658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = data.take(train_size)\nvalidation_data = data.skip(train_size).take(val_size) #skip already used data\ntest_data = data.skip(train_size+val_size).take(test_size) #skip already used data","metadata":{"execution":{"iopub.status.busy":"2024-07-26T07:35:30.058396Z","iopub.execute_input":"2024-07-26T07:35:30.058779Z","iopub.status.idle":"2024-07-26T07:35:30.073621Z","shell.execute_reply.started":"2024-07-26T07:35:30.058748Z","shell.execute_reply":"2024-07-26T07:35:30.072687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Model Building","metadata":{}},{"cell_type":"markdown","source":"## 2-1. Convolutional Neural Network","metadata":{}},{"cell_type":"markdown","source":"Having learned about CNN on [Computer Vision](https://www.kaggle.com/learn/computer-vision), I first tried to apply CNN on my own. The following is the entire code.","metadata":{}},{"cell_type":"code","source":"from keras import layers\nmodel=keras.Sequential([\n    # Base\n    layers.Conv2D(filters=64, kernel_size=(5, 5), padding='same',\n                 activation='relu', input_shape=[256,256,3]),\n    layers.MaxPool2D(pool_size=(2, 2)),\n\n    layers.Conv2D(filters=64, kernel_size=(5, 5), padding='same',\n                 activation='relu'),\n    layers.MaxPool2D(pool_size=(2, 2)),\n\n    layers.Conv2D(filters=64, kernel_size=(5, 5), padding='same',\n                 activation='relu'),\n    layers.MaxPool2D(pool_size=(2, 2)),\n\n    layers.Conv2D(filters=64, kernel_size=(5, 5), padding='same',\n                 activation='relu'),\n    layers.MaxPool2D(pool_size=(2, 2)),\n\n    # Head\n    layers.Flatten(),\n    layers.Dense(6, activation='relu'),\n    layers.Dense(5, activation='softmax')\n\n])\nmodel.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy'],\n)\n\nmodel.summary()\n\n## This is the interim model version used during the development process.\n'''\nhistory = model.fit(\n    train_data,\n    validation_data=validation_data,\n    epochs=30,\n)\n\n# Plot learning curves\nimport pandas as pd\nhistory_frame = pd.DataFrame(history.history)\nhistory_frame.loc[:, ['loss', 'val_loss']].plot()\nhistory_frame.loc[:, ['accuracy', 'val_accuracy']].plot();\n\nprint(\"Minimum validation loss: {}\".format(history_frame['val_loss'].min()))\nprint(\"Maximum accuracy: {}\".format(history_frame['val_accuracy'].max()))\n'''","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-07-26T09:45:28.182809Z","iopub.execute_input":"2024-07-26T09:45:28.183522Z","iopub.status.idle":"2024-07-26T09:45:28.936770Z","shell.execute_reply.started":"2024-07-26T09:45:28.183491Z","shell.execute_reply":"2024-07-26T09:45:28.935898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"While I increased the number of layers to make a complex model, it didn't perform well. Thinking this was due to the small size of the dataset, I decided to try different methods.","metadata":{}},{"cell_type":"markdown","source":"## 2-2. Transfer Learning ","metadata":{}},{"cell_type":"markdown","source":"Realizing that my dataset was too small to train a model from scratch, I decided to use a pretrained model (VGG16) and implement data augmentation.","metadata":{}},{"cell_type":"markdown","source":"### 2-2-1 Initial Model","metadata":{}},{"cell_type":"code","source":"from keras.applications import VGG16\n\n\npretrained_base = VGG16(\n    include_top=False,\n    input_shape=(256, 256, 3),\n    weights='imagenet',\n    pooling='max',\n    classifier_activation='softmax',\n)\npretrained_base.trainable = False\n\nmodel = keras.Sequential([\n    # Preprocessing\n    layers.RandomFlip('horizontal'), # flip left-to-right\n    layers.RandomRotation(factor=0.20),\n    \n    # Base\n    pretrained_base,\n    \n    # Head\n    layers.Flatten(),\n    layers.Dense(128, activation='relu'),  # Increased size\n    layers.Dense(64, activation='relu'),  # Additional dense layer\n    layers.Dense(5, activation='softmax')  # Output layer for 5 classes\n])","metadata":{"execution":{"iopub.status.busy":"2024-07-26T07:33:13.524181Z","iopub.execute_input":"2024-07-26T07:33:13.524554Z","iopub.status.idle":"2024-07-26T07:33:14.420851Z","shell.execute_reply.started":"2024-07-26T07:33:13.524515Z","shell.execute_reply":"2024-07-26T07:33:14.419875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(\n    min_delta=0.001, # minimium amount of change to count as an improvement\n    patience=20, # how many epochs to wait before stopping\n    restore_best_weights=True,\n)\nmodel.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n    loss='categorical_crossentropy',\n    metrics=['accuracy'],\n)\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-07-26T07:33:26.127677Z","iopub.execute_input":"2024-07-26T07:33:26.128062Z","iopub.status.idle":"2024-07-26T07:33:26.154922Z","shell.execute_reply.started":"2024-07-26T07:33:26.128030Z","shell.execute_reply":"2024-07-26T07:33:26.154089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This is the interim model version used during the development process.\n'''\nhistory = model.fit(\n    train_data,\n    validation_data=validation_data,\n    epochs=50,\n    callbacks=[early_stopping],\n)\n# Plot learning curves\nimport pandas as pd\nhistory_frame = pd.DataFrame(history.history)\nhistory_frame.loc[:, ['loss', 'val_loss']].plot()\nhistory_frame.loc[:, ['accuracy', 'val_accuracy']].plot();\n\nprint(\"Minimum validation loss: {}\".format(history_frame['val_loss'].min()))\nprint(\"Maximum accuracy: {}\".format(history_frame['val_accuracy'].max()))\n\n'''","metadata":{"execution":{"iopub.status.busy":"2024-07-11T00:16:44.243158Z","iopub.execute_input":"2024-07-11T00:16:44.243775Z","iopub.status.idle":"2024-07-11T00:22:46.140099Z","shell.execute_reply.started":"2024-07-11T00:16:44.243743Z","shell.execute_reply":"2024-07-11T00:22:46.139214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"While training reached around 85% of validation accuracy with these methods, it was still volatile. To stabilize the training and aim for higher accuracy, I did the following:\n\n1. Tried different learning rates:\n\n* With a too small learning rate, the training started too slowly.\n* With a too large learning rate, the training became too volatile towards the end.\n* To address this, I introduced a learning rate schedule.\n\n2. Tried different batch sizes:\n\n* With a too small batch size, the training became too volatile.\n* With a too large batch size, overfitting occurred.\n\n3. Tried different numbers of epochs.\n\n* Added data augmentation layers.\n\nAlong the way, both overfitting and underfitting occurred, so I added dropout layers but eventually commented them out. If the validation accuracy is consistently higher than the training accuracy, it indicates underfitting. Since this was the case, I removed the dropout layers.","metadata":{}},{"cell_type":"markdown","source":"### 2-2-2 Final Model","metadata":{}},{"cell_type":"code","source":"pretrained_base = VGG16(\n    include_top=False,\n    input_shape=(256, 256, 3),\n    weights='imagenet',\n    pooling='max',\n    classifier_activation='softmax',\n)\npretrained_base.trainable = False\n\nmodel = keras.Sequential([\n    # Preprocessing\n    layers.RandomFlip('horizontal'), # flip left-to-right\n    layers.RandomRotation(factor=0.20),\n    layers.RandomZoom(0.20),\n    \n    # Base\n    pretrained_base,\n    \n    # Head\n    layers.Flatten(),\n    layers.Dense(256, activation='relu'),\n    layers.BatchNormalization(),  # Add batch normalization\n    #layers.Dropout(0.5),\n    layers.Dense(128, activation='relu'),  # Increased size\n    layers.BatchNormalization(),\n    #layers.Dropout(0.5),\n    layers.Dense(64, activation='relu'),  # Additional dense layer\n    layers.BatchNormalization(),\n    #layers.Dropout(0.5),\n    layers.Dense(5, activation='softmax')  # Output layer for 5 classes\n])","metadata":{"execution":{"iopub.status.busy":"2024-07-26T07:56:55.269927Z","iopub.execute_input":"2024-07-26T07:56:55.270736Z","iopub.status.idle":"2024-07-26T07:56:55.540646Z","shell.execute_reply.started":"2024-07-26T07:56:55.270703Z","shell.execute_reply":"2024-07-26T07:56:55.539638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=0.01),\n    loss='categorical_crossentropy',\n    metrics=['accuracy'],\n)\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-07-26T07:56:57.648047Z","iopub.execute_input":"2024-07-26T07:56:57.648785Z","iopub.status.idle":"2024-07-26T07:56:57.676653Z","shell.execute_reply.started":"2024-07-26T07:56:57.648752Z","shell.execute_reply":"2024-07-26T07:56:57.675811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\n\nearly_stopping = EarlyStopping(\n    min_delta=0.001, # minimium amount of change to count as an improvement\n    patience=20, # how many epochs to wait before stopping\n    restore_best_weights=True,\n)\n\n\nreduce_lr = ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.5,\n    patience=5,\n    min_lr=0.00001\n)\n\n\nhistory = model.fit(\n    train_data,\n    validation_data=validation_data,\n    batch_size=128,\n    epochs=200,\n    callbacks=[early_stopping, reduce_lr],\n)\n\n# Plot learning curves\nimport pandas as pd\nhistory_frame = pd.DataFrame(history.history)\nhistory_frame.loc[:, ['loss', 'val_loss']].plot()\nhistory_frame.loc[:, ['accuracy', 'val_accuracy']].plot();","metadata":{"execution":{"iopub.status.busy":"2024-07-26T07:57:00.707418Z","iopub.execute_input":"2024-07-26T07:57:00.707786Z","iopub.status.idle":"2024-07-26T08:05:19.150828Z","shell.execute_reply.started":"2024-07-26T07:57:00.707758Z","shell.execute_reply":"2024-07-26T08:05:19.149882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Minimum validation loss: {}\".format(history_frame['val_loss'].min()))\nprint(\"Maximum accuracy: {}\".format(history_frame['val_accuracy'].max()))","metadata":{"execution":{"iopub.status.busy":"2024-07-26T08:10:56.211526Z","iopub.execute_input":"2024-07-26T08:10:56.211908Z","iopub.status.idle":"2024-07-26T08:10:56.218039Z","shell.execute_reply.started":"2024-07-26T08:10:56.211880Z","shell.execute_reply":"2024-07-26T08:10:56.217102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Evaluation","metadata":{}},{"cell_type":"code","source":"from keras.metrics import Precision, Recall, BinaryAccuracy\n\npre = Precision()\nre = Recall()\nacc = BinaryAccuracy()","metadata":{"execution":{"iopub.status.busy":"2024-07-26T08:11:00.461959Z","iopub.execute_input":"2024-07-26T08:11:00.462656Z","iopub.status.idle":"2024-07-26T08:11:00.476128Z","shell.execute_reply.started":"2024-07-26T08:11:00.462623Z","shell.execute_reply":"2024-07-26T08:11:00.475205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for batch in test_data.as_numpy_iterator():\n    X,y = batch\n    yhat = model.predict(X)\n    pre.update_state(y, yhat)\n    re.update_state(y, yhat)\n    acc.update_state(y, yhat)\n\nprint(f'Precision{pre.result().numpy()}, Recall:{re.result().numpy()}, Accuracy:{acc.result().numpy()}')","metadata":{"execution":{"iopub.status.busy":"2024-07-26T08:11:02.194766Z","iopub.execute_input":"2024-07-26T08:11:02.195119Z","iopub.status.idle":"2024-07-26T08:11:05.177920Z","shell.execute_reply.started":"2024-07-26T08:11:02.195094Z","shell.execute_reply":"2024-07-26T08:11:05.176942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### *Experiment with Images","metadata":{}},{"cell_type":"code","source":"import cv2\nimg = cv2.imread('/kaggle/input/hydrangea-experiment/12780.jpg')\nplt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-26T08:11:09.543642Z","iopub.execute_input":"2024-07-26T08:11:09.544351Z","iopub.status.idle":"2024-07-26T08:11:09.905347Z","shell.execute_reply.started":"2024-07-26T08:11:09.544306Z","shell.execute_reply":"2024-07-26T08:11:09.904412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resize = tensorflow.image.resize(img, (256, 256))\nplt.imshow(resize.numpy().astype(int))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-26T08:11:13.855152Z","iopub.execute_input":"2024-07-26T08:11:13.855944Z","iopub.status.idle":"2024-07-26T08:11:14.107576Z","shell.execute_reply.started":"2024-07-26T08:11:13.855909Z","shell.execute_reply":"2024-07-26T08:11:14.106595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.expand_dims(resize,0)\nyhat = model.predict(np.expand_dims(resize/255,0))","metadata":{"execution":{"iopub.status.busy":"2024-07-26T08:11:16.576394Z","iopub.execute_input":"2024-07-26T08:11:16.576747Z","iopub.status.idle":"2024-07-26T08:11:16.677682Z","shell.execute_reply.started":"2024-07-26T08:11:16.576721Z","shell.execute_reply":"2024-07-26T08:11:16.676697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert probabilities to class name\npredicted_class_index = np.argmax(yhat)  # Get the index of the class with the highest probability\npredicted_class_name = class_names[predicted_class_index]  # Map index to class name\n\nprint(f\"Predicted class: {predicted_class_name}\")\nprint(f\"Probabilities: {yhat}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-26T08:11:20.447676Z","iopub.execute_input":"2024-07-26T08:11:20.448272Z","iopub.status.idle":"2024-07-26T08:11:20.453976Z","shell.execute_reply.started":"2024-07-26T08:11:20.448239Z","shell.execute_reply":"2024-07-26T08:11:20.453104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Save the Model","metadata":{}},{"cell_type":"code","source":"model.save('hydrangea_model.keras')","metadata":{"execution":{"iopub.status.busy":"2024-07-26T08:11:24.144363Z","iopub.execute_input":"2024-07-26T08:11:24.144751Z","iopub.status.idle":"2024-07-26T08:11:24.465174Z","shell.execute_reply.started":"2024-07-26T08:11:24.144721Z","shell.execute_reply":"2024-07-26T08:11:24.464290Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loaded_model = keras.saving.load_model('/kaggle/working/hydrangea_model.keras')","metadata":{"execution":{"iopub.status.busy":"2024-07-26T08:11:52.315011Z","iopub.execute_input":"2024-07-26T08:11:52.315741Z","iopub.status.idle":"2024-07-26T08:11:54.138933Z","shell.execute_reply.started":"2024-07-26T08:11:52.315709Z","shell.execute_reply":"2024-07-26T08:11:54.138121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for batch in test_data.as_numpy_iterator():\n    X,y = batch\n    yhat = loaded_model.predict(X)\n    pre.update_state(y, yhat)\n    re.update_state(y, yhat)\n    acc.update_state(y, yhat)\n\nprint(f'Precision{pre.result().numpy()}, Recall:{re.result().numpy()}, Accuracy:{acc.result().numpy()}')","metadata":{"execution":{"iopub.status.busy":"2024-07-26T08:11:56.379577Z","iopub.execute_input":"2024-07-26T08:11:56.380541Z","iopub.status.idle":"2024-07-26T08:11:59.301438Z","shell.execute_reply.started":"2024-07-26T08:11:56.380506Z","shell.execute_reply":"2024-07-26T08:11:59.300510Z"},"trusted":true},"execution_count":null,"outputs":[]}]}